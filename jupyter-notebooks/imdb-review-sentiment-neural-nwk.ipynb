{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Sentiment analysis on IMDB reviews\n\n## Loading data\n\nWe will first load the [publicly available dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) using the following code - "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.datasets import imdb\n\ntop_words = 10000\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let us now inspect the training data and we can go by the hypothesis test data will not be vastly different in nature than the train_data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "(x_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The first thing we notice here is, there is no text! It is all numbers. It is so, because neural networks work with numbers and not text. Thus, the dataset has already encoded the data the text in numbers. The way this encoding has happened is narrated in this picture - \n\n<img src=\"https://docs.microsoft.com/en-us/learn/student-evangelism/analyze-review-sentiment-with-keras/media/2-keras-docs.png\" alt=\"Keras dataset documentation on IMDB reviews\" />\n\nAs we realize by now these encoded text, let us take a look at the dictionary"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "imdb.get_word_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let us now get the word corresponding to any code; i.e. reverse encode the value based on the number"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_reverse_encode() :\n    word_dict = imdb.get_word_index()\n    word_dict = {key:(value+3) for key, value in word_dict.items()}\n    word_dict[''] = 0 #Padding at the start\n    word_dict['>'] = 1 #Starting of the review\n    word_dict['?'] = 2 #Unknown word\n    reverse_word_dict = {value:key for key, value in word_dict.items()}\n    return reverse_word_dict\n\nprint(' '.join(get_reverse_encode()[id] for id in x_train[0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing import sequence\nmax_review_length = 500\nx_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\nx_test = sequence.pad_sequences(x_test, maxlen=max_review_length)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the line above we have ensure all reviews are of constant length 500 words with reviews which are shorter are padded with 0 which is a reserved word for the operation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten\n\n\nembedding_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Code above constructs the neural network i.e. configures the neural network. It defines the layers and its annexed configuration. Here we have configured 5 layers. The layer that we start with is Embedding. The layer is critical for any neural network that intends to process word corpus. Functionally the layer maps a multidimensional large integer baesd array to a restricted floating point array which subsequent layers can process easily. Embedding is followed by Flatten and few Dense layer. Flatten layer is bridge that translates output of Embedding to layers that follows.\nThe Dense layers are fully connected neurons of size 16. These happen to be basic kind of neural network with 16 neurons. The number 16 is arbitrary however, we will have to tune the numbers by inspecting the results of training."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=2, batch_size=128)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "BY calling the fit function we are training the network on the training data. Training is repeated 5 times which is known as epoch. During the run of training the model passes data back and forth through the neurons to tune the parameters. For each epoch an accuracy score is generated. Though during 1 epoch the validation accuracy increases but a higher value for both training accuracy and validation accuracy increases the risk of overfitting a model. To visualize this risk let us plot these values through epochs"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as mpl\nimport seaborn as sns\n\nsns.set()\n\nacc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\nepochs = range(1, len(acc)+1)\n\nmpl.plot(epochs, acc, '-', label='Training accuracy')\nmpl.plot(epochs, val_acc, ':', label='Validation accuracy')\nmpl.title('Training vs Validation accuracy')\nmpl.xlabel('Epoch')\nmpl.ylabel('Accuracy')\nmpl.legend(loc='upper left')\nmpl.plot()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let us make observation from this graph but progress to make prediction and judge the accuracy of such predictions. Let us not forget to remind ourselves prediction accuracy is just another metric of model performing. It should not be used in isolation to select a model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scores = model.evaluate(x_test, y_test)\nprint('Model accuracy - %.2f%%' % (scores[1]*100))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}